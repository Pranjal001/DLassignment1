# -*- coding: utf-8 -*-
"""DLA1beta2.4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11oDfhnfCt71TO9mgSUbaQyrhvys2aMAS
"""
import wandb
import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import fashion_mnist , mnist
from sklearn.model_selection import train_test_split
import argparse

# wandb.init(project="DL_assignment_1", name = "Question 1")

# Load the Fashion-MNIST dataset
(x_train, y_train), (x_test,y_test) = fashion_mnist.load_data()


# Define class labels
class_labels = {
    0: 'T-shirt',
    1: 'Trouser',
    2: 'Pullover',
    3: 'Dress',
    4: 'Coat',
    5: 'Sandal',
    6: 'Shirt',
    7: 'Sneaker',
    8: 'Bag',
    9: 'boot'
}

# Plot one sample image for each class
fig, axes = plt.subplots(2, 5, figsize=(12, 6))
axes = axes.ravel()

for i in range(10):
    # Find the first image of current class
    idx = np.where(y_train == i)[0][0]
    # Plot the image
    axes[i].imshow(x_train[idx], cmap='gray')
    axes[i].set_title(class_labels[i])
    axes[i].axis('off')
    # wandb.log({"Question 1": [wandb.Image(axes[i], caption=y_train[i])]})

plt.tight_layout()
plt.show()
# wandb.finish()

#x_train.shape

"""Question 2 : implement a feed forward network



"""

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ NEURAL NETWORK ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
class NeuralNet:
  def __init__(self, par):

    #defining required paramters within this class
    self.layer_data = {}
    self.epoch = par['epochs']
    self.hiddenlayers = par['hidden_layers']
    self.hiddenlayer_size = par['hidden_layer_size']
    self.inputSize = par['input_size']
    self.outputSize = par['output_size']
    self.eta = par['learning_rate']
    self.beta = par['beta']
    self.momentum = par['momentum']
    self.beta1 = par['beta1']
    self.beta2 = par['beta2']
    self.epsilon = par['epsilon']
    self.optimizer = par['optimizer']
    self.batch = par['batch_size']
    self.activation = par['activation']
    self.initialiser = par['initialiser']
    self.weight_decay = par['weight_decay']
    self.loss_type = par['loss_type']
    self.a = {}
    self.h = {}
    self.dw = {}
    self.db = {}

    L = self.hiddenlayers + 1 # 1- input L - hidden 1 - output
    self.layer_data[1] = weightsAndBiasLayer(self.inputSize,self.hiddenlayer_size,self.initialiser)

    for i in range(2 , L):
      self.layer_data[i] = weightsAndBiasLayer(self.hiddenlayer_size,self.hiddenlayer_size,self.initialiser)

    self.layer_data[L] = weightsAndBiasLayer(self.hiddenlayer_size,self.outputSize,self.initialiser)


#------------------------------------ forward propagation---------------------------------------------------


  def forwardpropagate(self,x):

    self.h[0] = x
    L  = len(self.layer_data)
    for i in range(1, L):
      self.a[i] = self.layer_data[i].forwarding(self.h[i-1])
      self.h[i] = self.layer_data[i].activation_function(self.activation, self.a[i])
    self.a[L] = self.layer_data[L].forwarding(self.h[L-1])
    y_pred = self.layer_data[L].activation_function('softmax',self.a[L])
    return y_pred


#------------------------------------backward propagation---------------------------------------------------

  def backwardpropagate(self, y_pred, y_true):

    L = len(self.layer_data)
    n = len(y_pred)

    #handling output loss for MSE and CE

    if self.loss_type == 'cross_entropy':
      op_error = y_pred - y_true

    if self.loss_type == 'mean_squared_error':
      op_error = (y_pred - y_true)*self.layer_data[L].diff_function('softmax',self.a[L]) / (y_pred.shape[0])

    for i in range(L, 1, -1):
      self.dw[i] = np.dot(self.h[i-1].T,op_error) /n
      self.db[i] = np.sum(op_error, axis=0, keepdims=True)/n

      grad_layer = np.dot(op_error , self.layer_data[i].weights.T)
      op_error = grad_layer * self.layer_data[i-1].diff_function(self.activation,self.a[i-1])

    # 1
    self.dw[1] = np.dot(self.h[0].T,op_error)/n
    self.db[1] = np.sum(op_error, axis=0, keepdims=True)/n



# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LAYER CLASS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
class weightsAndBiasLayer:


#------------------------------------weight initialisation method---------------------------------------------------

  def __init__(self,lowlayer,upperlayer,initialiser):

    if(initialiser == 'random'):
      self.weights = np.random.randn(lowlayer,upperlayer)
      self.bias = np.random.randn(1,upperlayer)
    if(initialiser == 'Xavier'):
      variance = 6.0 / (lowlayer + upperlayer)
      stddev = np.sqrt(variance)
      self.weights = np.random.randn(lowlayer,upperlayer)*stddev
      self.bias = np.random.randn(1,upperlayer)*stddev

  def forwarding(self,x):
    return np.dot(x,self.weights) + self.bias

#----------------------------------------------Activation funciton-------------------------------------

  def activation_function(self,func_type,x):
    if(func_type == 'sigmoid'):
      return self.sigmoid(x)
    elif(func_type == 'softmax'):
      return self.softmax(x)
    if(func_type == 'tanh'):
      return self.tanh(x)
    if(func_type == 'ReLU'):
      return self.reLu(x)
    if(func_type == 'identity'):
      return self.identity


  def sigmoid(self,input):

    return 1.0/(1.0 + np.exp(-input))

  def softmax(self,x):
    exp_vals = np.exp(x - np.max(x, axis=-1 , keepdims = True))
    softmax_vals = exp_vals / np.sum(exp_vals, axis=-1 , keepdims = True)
    return softmax_vals

  def tanh(self,x):

    tan_res =np.tanh(x)
    return tan_res

  def reLu(self,x):

    return np.maximum(0,x)

  def identity(self,x):
    return x

#-----------------------------------------------Diffrentiation of activation funcitons------------------------------------
  def diff_function(self,func_type,x):
    if(func_type == 'sigmoid'):
      return self.diff_sig(x)
    if(func_type == 'tanh'):
      return self.diff_tanh(x)
    if(func_type == 'ReLU'):
      return self.diff_reLu(x)
    if(func_type == 'identity'):
      return self.diff_identity(x)
    if(func_type == 'softmax'):
      return self.diff_softmax(x)

  def diff_sig(self, x):
    func = self.sigmoid(x)
    return func*(1-func)

  def diff_tanh(self,x):
    func = self.tanh(x)
    return 1 - func**2

  def diff_reLu(self,x):
    return np.where(x <= 0, 0, 1)

  def diff_identity(self,x):
    return np.ones(x.shape)

  def diff_softmax(self,x):
      g = self.softmax(x)
      return g * (1 - g)

#------------------------------------------optimisation functions-----------------------------------------
class optimisation_functions:

  def __init__(self,neural_net,opt_func,eta_par,beta_par,momentum_par,epsilon,beta1,beta2,weight_decay):
    self.opt_type = opt_func
    self.neural_net = neural_net
    self.eta = eta_par
    self.beta = beta_par
    self.momentum = momentum_par
    self.beta1 = beta1
    self.beta2 = beta2
    self.epsilon = epsilon
    self.decay = weight_decay
    self.u_wt = {}
    self.u_b = {}
    self.vw = {}
    self.vb = {}
    self.mw = {}
    self.mb = {}
    self.mcap_w = {}
    self.mcap_b = {}
    self.vcap_w = {}
    self.vcap_b = {}
    self.mform_w = {}
    self.mform_b = {}
    self.t = 1


  def update_rule(self):
    if(self.opt_type == 'sgd'):
      self.sgd()
    if(self.opt_type == 'mgd'):
      self.mgd()
    if(self.opt_type == 'nag'):
      self.nag()
    if(self.opt_type == 'rmsprop'):
      self.rms_prop()
    if(self.opt_type == 'adam'):
      self.adam()
    if(self.opt_type == 'nadam'):
      self.nadam()


  def sgd(self):

    for i in range(1, len(self.neural_net.layer_data) + 1):
      self.neural_net.dw[i] += self.neural_net.layer_data[i].weights*self.decay
      self.neural_net.layer_data[i].weights -= (self.eta*self.neural_net.dw[i])
      self.neural_net.layer_data[i].bias -= (self.eta*self.neural_net.db[i])

  def mgd(self):
    if(len(self.u_wt)  == 0 ):
      self.u_wt = self.neural_net.dw
      self.u_b = self.neural_net.db

    for i in range(1,len(self.neural_net.layer_data)+1):

      self.neural_net.dw[i] += self.neural_net.layer_data[i].weights*self.decay
      self.u_wt[i] = self.momentum * self.u_wt[i] + self.neural_net.dw[i]
      self.u_b[i] = self.momentum * self.u_b[i] + self.neural_net.db[i]
      self.neural_net.layer_data[i].weights -= self.eta*self.u_wt[i]
      self.neural_net.layer_data[i].bias -= self.eta*self.u_b[i]

  def nag(self):
# this formation of Nag was taken from later slides(Sir's slides) to avoid calculating grad(w-vw)
    if(len(self.vw )== 0):
      for i in range(1,len(self.neural_net.layer_data)+1):

        self.vw[i] = np.zeros(self.neural_net.dw[i].shape)
        self.vb[i] = np.zeros(self.neural_net.db[i].shape)

    for i in range(1,len(self.neural_net.layer_data)+1):

      self.neural_net.dw[i] += self.neural_net.layer_data[i].weights*self.decay
      self.vw[i] = self.beta * self.vw[i] + self.neural_net.dw[i]
      self.vb[i] = self.beta * self.vb[i] + self.neural_net.db[i]

      self.neural_net.layer_data[i].weights -= self.eta*(self.vw[i] + self.neural_net.dw[i])
      self.neural_net.layer_data[i].bias -= self.eta*(self.vb[i] + self.neural_net.db[i])

  def rms_prop(self):

    if(len(self.vw )== 0):
      for i in range(1,len(self.neural_net.layer_data)+1):

        self.vw[i] = (self.neural_net.dw[i]**2)
        self.vb[i] = (self.neural_net.db[i]**2)

    for i in range(1,len(self.neural_net.layer_data)+1):

      self.neural_net.dw[i] += self.neural_net.layer_data[i].weights*self.decay

      self.vw[i] = self.beta * self.vw[i] + (1 - self.beta)*(self.neural_net.dw[i]**2)
      self.vb[i] = self.beta * self.vb[i] + (1 - self.beta)*(self.neural_net.db[i]**2)

      self.neural_net.layer_data[i].weights -= (self.eta/np.sqrt(self.vw[i] + self.epsilon))*(self.vw[i] + self.neural_net.dw[i])
      self.neural_net.layer_data[i].bias -= (self.eta/np.sqrt(self.vb[i] + self.epsilon))*(self.vb[i] + self.neural_net.db[i])

  def adam(self):

    if(len(self.vw )== 0):
      for i in range(1,len(self.neural_net.layer_data)+1):

        self.vw[i] = (self.neural_net.dw[i]**2)
        self.vb[i] = (self.neural_net.db[i]**2)

    if(len(self.mw )== 0):
      for i in range(1,len(self.neural_net.layer_data)+1):

        self.mw[i] = (self.neural_net.dw[i])
        self.mb[i] = (self.neural_net.db[i])

    for i in range(1,len(self.neural_net.layer_data)+1):

      self.neural_net.dw[i] += self.neural_net.layer_data[i].weights*self.decay

      self.mw[i] = self.beta1 * self.mw[i] + (1 - self.beta1)*(self.neural_net.dw[i])
      self.mb[i] = self.beta1 * self.mb[i] + (1 - self.beta1)*(self.neural_net.db[i])

      self.mcap_w[i] = self.mw[i] / (1 - self.beta1**self.t)
      self.mcap_b[i] = self.mb[i] / (1 - self.beta1**self.t)

      self.vw[i] = self.beta2 * self.vw[i] + (1 - self.beta2)*(self.neural_net.dw[i]**2)
      self.vb[i] = self.beta2 * self.vb[i] + (1 - self.beta2)*(self.neural_net.db[i]**2)

      self.vcap_w[i] = self.vw[i] / (1 - self.beta2**self.t)
      self.vcap_b[i] = self.vb[i] / (1 - self.beta2**self.t)

      self.neural_net.layer_data[i].weights -= (self.eta/np.sqrt(self.vcap_w[i] + self.epsilon))*(self.mcap_w[i])
      self.neural_net.layer_data[i].bias -= (self.eta/np.sqrt(self.vcap_b[i] + self.epsilon))*(self.mcap_b[i])

    self.t += 1

  def nadam(self):

    if(len(self.vw )== 0):
      for i in range(1,len(self.neural_net.layer_data)+1):

        self.vw[i] = (self.neural_net.dw[i]**2)
        self.vb[i] = (self.neural_net.db[i]**2)

    if(len(self.mw )== 0):
      for i in range(1,len(self.neural_net.layer_data)+1):

        self.mw[i] = (self.neural_net.dw[i])
        self.mb[i] = (self.neural_net.db[i])

    for i in range(1,len(self.neural_net.layer_data)+1):

      self.neural_net.dw[i] += self.neural_net.layer_data[i].weights*self.decay

      self.mw[i] = self.beta1 * self.mw[i] + (1 - self.beta1)*(self.neural_net.dw[i])
      self.mb[i] = self.beta1 * self.mb[i] + (1 - self.beta1)*(self.neural_net.db[i])

      self.mcap_w[i] = self.mw[i] / (1 - self.beta1**self.t)
      self.mcap_b[i] = self.mb[i] / (1 - self.beta1**self.t)

      self.vw[i] = self.beta2 * self.vw[i] + (1 - self.beta2)*(self.neural_net.dw[i]**2)
      self.vb[i] = self.beta2 * self.vb[i] + (1 - self.beta2)*(self.neural_net.db[i]**2)

      self.vcap_w[i] = self.vw[i] / (1 - self.beta2**self.t)
      self.vcap_b[i] = self.vb[i] / (1 - self.beta2**self.t)

      self.mform_w[i] = self.beta1 * self.mcap_w[i] + (1 - self.beta1)*self.neural_net.dw[i]
      self.mform_b[i] = self.beta1 * self.mcap_b[i] + (1 - self.beta1)*self.neural_net.db[i]

      self.neural_net.layer_data[i].weights -= (self.eta/np.sqrt(self.vcap_w[i] + self.epsilon))*(self.mform_w[i])
      self.neural_net.layer_data[i].bias -= (self.eta/np.sqrt(self.vcap_b[i] + self.epsilon))*(self.mform_b[i])

    self.t += 1



#============================================Super Neural (Calls and trains the entire Neural Network)=================================================


class super_neural:

  def __init__(self,parameters,X_train, Y_train, X_test, Y_test,x_val , y_val):
    self.nn = NeuralNet(parameters)
    self.opt_call = opt_call = optimisation_functions(self.nn,self.nn.optimizer,self.nn.eta,self.nn.beta,self.nn.momentum,self.nn.epsilon,self.nn.beta1,self.nn.beta2,self.nn.weight_decay)
    self.train_neural(X_train, Y_train, X_test, Y_test , x_val , y_val)


  def cross_entropy(self, y_true, y_pred):
    y_pred = np.clip(y_pred, 1e-15, 1. - 1e-15)
    loss = -np.sum(y_true * np.log(y_pred), axis=1)
    loss = np.mean(loss)
    return loss

  def mean_squared_error(self,true_values, predicted_values):

    squared_errors = 0.5*(true_values - predicted_values) ** 2
    mean_squared_error = np.mean(squared_errors)

    return mean_squared_error


  def accuracy(self,y_true ,y_pred):
    pred = np.argmax(y_pred, axis=1)
    true = np.argmax(y_true, axis=1)
    predict = np.sum(pred == true)
    return (predict/len(pred))*100


  def loss_call(self,y_true, y_pred):
    if( self.nn.loss_type == 'mean_squared_error'):
      return self.mean_squared_error(y_true, y_pred)
    if( self.nn.loss_type == 'cross_entropy'):
      return self.cross_entropy(y_true, y_pred)


# funciton which plots a confusion matrix

  def plot_confusion_matrix(self,true_labels, predicted_labels):

    classes = ['T-shirt','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','boot']
    # Compute confusion matrix
    true_labels = np.argmax(true_labels, axis=1)
    predicted_labels = np.argmax(predicted_labels, axis=1)
    num_classes = len(classes)
    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)
    for i in range(len(true_labels)):
        true_class = true_labels[i]
        predicted_class = predicted_labels[i]
        confusion_matrix[int(true_class)][int(predicted_class)] += 1

    # Plot confusion matrix
    plt.figure(figsize=(9, 11))
    plt.imshow(confusion_matrix, interpolation='nearest', cmap="turbo")
    plt.title('Confusion Matrix')
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes)
    plt.yticks(tick_marks, classes)

    thresh = confusion_matrix.max() / 2.
    for i in range(num_classes):
        for j in range(num_classes):
            plt.text(j, i, format(confusion_matrix[i, j], 'd'),
                     horizontalalignment="center",
                     color="white" if confusion_matrix[i, j] > thresh else "white")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    wandb.init(project="DL_assignment_1",id="confusion_matrix")
    images = wandb.Image(plt)
    wandb.log({"confusion_matrix":images})
    plt.show()


#------------------------------------trians the network with customized batch sizes---------------------------------------------------


  def train_neural(self, X_train, Y_train , X_test ,Y_test,x_val , y_val):

    for epochit in range(self.nn.epoch):

      for batch_beg in range(0,len(X_train),self.nn.batch):

        batch_end = batch_beg+self.nn.batch
        input_data = X_train[batch_beg:batch_end]
        output_data = Y_train[batch_beg:batch_end]

        y_pred = self.nn.forwardpropagate(input_data)

        self.nn.backwardpropagate(y_pred, output_data)
        self.opt_call.update_rule()

      y_pred = self.nn.forwardpropagate(X_train)
      y_pred_test = self.nn.forwardpropagate(X_test)
      y_pred_val = self.nn.forwardpropagate(x_val)
      print(f"Epoch : {epochit + 1} Loss : {self.loss_call(Y_train, y_pred)} Accuracy : {self.accuracy(Y_train, y_pred)}")
      print("------------------------------------------------------------------------------------------------------------------------------")
      print(f"            Validation  Loss : {self.loss_call(y_val, y_pred_val)} Accuracy : {self.accuracy(y_val, y_pred_val)}")
      print("===============================================================================================================================")
      wandb.log({'epoch':epochit + 1, 'train_loss': self.loss_call(Y_train, y_pred), 'train_accuracy': self.accuracy(Y_train, y_pred), 'val_loss': self.cross_entropy(y_val, y_pred_val), 'val_accuracy': self.accuracy(y_val, y_pred_val)})

"""main

"""

#------------------------------------Defines sweep configs for Wandb plotting---------------------------------------------------



sweep_config = {
    'name' : "sweep_final_MNIST",
    'method': 'grid'
    }

parameters_sweep = {
  'epochs' : {'values' : [10] } , #5, number of epochs ,15
  'hidden_layers' : {'values' : [5] } , #4, hidden layers neuron count and number of hidden layers
  'hidden_layer_size' : {'values' : [128] } ,#32,64,
  'input_size' : {'values' : [784] } ,
  'output_size' : {'values' : [10] } ,
  'learning_rate' : {'values' : [0.0001] } , #0.0005, eta
  'momentum' : {'values' : [0.009]}, #,0.5 beta for momentum
  'beta' : {'values' : [0.9] } ,  #,0.5 beta for rms
  'beta1' : {'values' : [0.9] }, #,0.999
  'beta2' : {'values' : [0.999] },
  'epsilon' : {'values' : [1e-6] },#1e-8,
  'optimizer' : {'values' : ['adam','nadam'] }, #'mgd','sgd','nag','rmsprop',,'adam' type of optimizer such as sgd,nadam,nag,
  'batch_size' : {'values' : [32] }, #16,32,32,,128
  'initialiser' : {'values' : ['Xavier'] }, #,'random' random,Xavier
  'activation' : {'values' : ['ReLU'] }, #'sigmoid','tanh', type of actvation
  'weight_decay' : {'values' : [1e-6,0]}, #0.0,
  'loss_type' : {'values' : ['cross_entropy']}, #,'mean_squared_error', 'cross_entropy'
  'dataset' : {'values' : ['mnist']} #mnist
  }


metric = {
    'name' : 'Accuracy',
    'goal' : 'maximize'
}

sweep_config['metric'] = metric

sweep_config['parameters'] = parameters_sweep

#------------------------------------Loads the type of dataset needed---------------------------------------------------

def dataset_selector(dataset):
  if(dataset == 'mnist'):
      (x_train, y_train), (x_test,y_test) = mnist.load_data()
      x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)
  else:
    (x_train, y_train), (x_test,y_test) = fashion_mnist.load_data()
    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)

  return x_train,y_train,x_val,y_val


def reshape_1D(data):
  return data.reshape(data.shape[0],-1)/255.0

def one_hot_vector(y):
  hot_vectors = np.zeros((len(y), 10))  # Initialize array with zeros
  hot_vectors[np.arange(len(y)), y] = 1  # Set the corresponding index to 1 for each one-hot vector
  return hot_vectors

    
parser = argparse.ArgumentParser()

parser.add_argument('-wp' , '--wandb_project', help='Project name used to track experiments in Weights & Biases dashboard' , type=str, default='DL_Assignment_1')
parser.add_argument('-we', '--wandb_entity' , help='Wandb Entity used to track experiments in the Weights & Biases dashboard.' , type=str, default='cs23m049')
parser.add_argument('-d', '--dataset', help='choices: ["mnist", "fashion_mnist"]', choices = ["mnist", "fashion_mnist"],type=str, default='fashion_mnist')
parser.add_argument('-e', '--epochs', help="Number of epochs to train neural network.", type=int, default=10)
parser.add_argument('-b', '--batch_size', help="Batch size used to train neural network.", type=int, default=64)
parser.add_argument('-l','--loss', help = 'choices: ["mean_squared_error", "cross_entropy"]' , choices = ["mean_squared_error", "cross_entropy"],type=str, default='cross_entropy')
parser.add_argument('-o', '--optimizer', help = 'choices: ["sgd", "momentum", "nag", "rmsprop", "adam", "nadam"]', choices = ["sgd", "momentum", "nag", "rmsprop", "adam", "nadam"],type=str, default = 'nadam')
parser.add_argument('-lr', '--learning_rate', help = 'Learning rate used to optimize model parameters', type=float, default=0.0009)
parser.add_argument('-m', '--momentum', help='Momentum used by momentum and nag optimizers.',type=float, default=0.9)
parser.add_argument('-beta', '--beta', help='Beta used by rmsprop optimizer',type=float, default=0.9)
parser.add_argument('-beta1', '--beta1', help='Beta1 used by adam and nadam optimizers.',type=float, default=0.9)
parser.add_argument('-beta2', '--beta2', help='Beta2 used by adam and nadam optimizers.',type=float, default=0.999)
parser.add_argument('-eps', '--epsilon', help='Epsilon used by optimizers.',type=float, default=1e-6)
parser.add_argument('-w_d', '--weight_decay', help='Weight decay used by optimizers.',type=float, default=0.0005)
parser.add_argument('-w_i', '--weight_init', help = 'choices: ["random", "Xavier"]', choices = ["random", "Xavier"],type=str, default='Xavier')
parser.add_argument('-nhl', '--num_layers', help='Number of hidden layers used in feedforward neural network.',type=int, default=5)
parser.add_argument('-sz', '--hidden_size', help ='Number of hidden neurons in a feedforward layer.', type=int, default=128)
parser.add_argument('-a', '--activation', help='choices: ["identity", "sigmoid", "tanh", "ReLU"]', choices = ["identity", "sigmoid", "tanh", "ReLU"],type=str, default='ReLU')
arguments = parser.parse_args()

wandb.init(project = arguments.wandb_project, name = arguments.wandb_entity)


parameters = {
                'dataset' : arguments.dataset,
                'epochs': arguments.epochs,
                'batch_size': arguments.batch_size,
                'loss_type': arguments.loss, # "mean_squared_error", "cross_entropy"
                'optimizer': arguments.optimizer, # "sgd", "momentum", "nag", "rmsprop", "adam", "nadam"
                'learning_rate': arguments.learning_rate,
                'momentum': arguments.momentum,
                'beta': arguments.beta,
                'beta1': arguments.beta1,
                'beta2': arguments.beta2,
                'epsilon': arguments.epsilon,
                'weight_decay': arguments.weight_decay,
                'initialiser': arguments.weight_init, # "random", "Xavier"
                'hidden_layers': arguments.num_layers,
                'hidden_layer_size': arguments.hidden_size,
                'activation': arguments.activation, # "identity", "sigmoid", "tanh", "ReLU"
                'input_size' : 784,
                'output_size' : 10,
                  }

#data preprocessing and dataset selection

x_train,y_train,x_val,y_val = dataset_selector(parameters['dataset'])

X_train = reshape_1D(x_train)
Y_train = one_hot_vector(y_train)
X_test = reshape_1D(x_test)
Y_test = one_hot_vector(y_test)
X_val = reshape_1D(x_val)
Y_val = one_hot_vector(y_val)

#model creation for the neural network and generating logs for accuracy along with confusion matrix

model = super_neural(parameters,X_train, Y_train, X_test, Y_test,X_val , Y_val)
y_pred_val = model.nn.forwardpropagate(X_val)
y_pred_test = model.nn.forwardpropagate(X_test)
acc_val = model.accuracy(y_pred_val , Y_val)
acc_test = model.accuracy(y_pred_test , Y_test)
wandb.log({'Accuracy': acc_val})
model.plot_confusion_matrix(y_pred_test,Y_test)

